{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef19bf0-3e0b-4ceb-9b88-733f31877f07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 09:58:28.972530: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-02 09:58:29.873335: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-02 09:58:31.635613: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/beckerf/mambaforge/envs/tiberiusdev/lib/:/home/beckerf/mambaforge/envs/tiberiusdev/lib/python3.10/site-packages/nvidia/cudnn/lib:\n",
      "2024-11-02 09:58:31.637259: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/beckerf/mambaforge/envs/tiberiusdev/lib/:/home/beckerf/mambaforge/envs/tiberiusdev/lib/python3.10/site-packages/nvidia/cudnn/lib:\n",
      "2024-11-02 09:58:31.637271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-11-02 09:58:34.337406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.337683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.358218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.358477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.358678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.358869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.365554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-02 09:58:34.477063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.477270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.477447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.477612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.477775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.477939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.493371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.493594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.494909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.495107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.495311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.495506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22463 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3e:00.0, compute capability: 8.6\n",
      "2024-11-02 09:58:34.497366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-02 09:58:34.497553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3f:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#temporary \n",
    "sys.path.insert(0, \"../../../learnMSA\")\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "sys.path.append(\"../../bin\")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as keras\n",
    "from eval_model_class import PredictionGTF\n",
    "from models import lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a6abab-e617-45da-a32a-84ae0494a484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 500004\n",
    "strand = '+'\n",
    "\n",
    "emb = False\n",
    "hmm_factor = 1\n",
    "hmm_parallel = 817\n",
    "trans_lstm = False\n",
    "\n",
    "inp_data_dir = 'inp/'\n",
    "out_dir = 'test_train/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "model_path = \"../../model_weights/tiberius_weights\"\n",
    "genome_path = f'{inp_data_dir}/genome.fa'\n",
    "# output gtf file\n",
    "gtf_out = 'tiberius.out'\n",
    "\n",
    "pred_gtf = PredictionGTF( \n",
    "    model_path=model_path,\n",
    "     seq_len=seq_len, \n",
    "    batch_size=batch_size,\n",
    "    hmm=True, \n",
    "    emb=False, \n",
    "    num_hmm=1,\n",
    "    hmm_factor=1,\n",
    "    genome_path=genome_path,\n",
    "    softmask=True, strand=strand,\n",
    "    parallel_factor=hmm_parallel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ebf06cc-df11-4c74-b25c-0dc0cb2e7439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gene pred hmm layer with parallel factor 817\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " main_input (InputLayer)        [(None, None, 6)]    0           []                               \n",
      "                                                                                                  \n",
      " initial_conv (Conv1D)          (None, None, 128)    2432        ['main_input[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization1 (LayerNor  (None, None, 128)   256         ['initial_conv[0][0]']           \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " conv_1 (Conv1D)                (None, None, 128)    147584      ['layer_normalization1[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization2 (LayerNor  (None, None, 128)   256         ['conv_1[0][0]']                 \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " conv_2 (Conv1D)                (None, None, 128)    147584      ['layer_normalization2[0][0]']   \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, None, 134)    0           ['main_input[0][0]',             \n",
      "                                                                  'conv_2[0][0]']                 \n",
      "                                                                                                  \n",
      " R1 (Reshape)                   (None, None, 1206)   0           ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " pre_lstm_dense (Dense)         (None, None, 744)    898008      ['R1[0][0]']                     \n",
      "                                                                                                  \n",
      " biLSTM_1 (Bidirectional)       (None, None, 744)    3324192     ['pre_lstm_dense[0][0]']         \n",
      "                                                                                                  \n",
      " biLSTM_2 (Bidirectional)       (None, None, 744)    3324192     ['biLSTM_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 270)    201150      ['biLSTM_2[0][0]']               \n",
      "                                                                                                  \n",
      " Reshape2 (Reshape)             (None, None, 30)     0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, None, 158)    0           ['Reshape2[0][0]',               \n",
      "                                                                  'conv_2[0][0]']                 \n",
      "                                                                                                  \n",
      " out_dense (Dense)              (None, None, 15)     2385        ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " out (Activation)               (None, None, 15)     0           ['out_dense[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, None, 5)     0           ['main_input[0][0]']             \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " lstm_out (Lambda)              (None, None, 15)     0           ['out[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None, None, 5)      0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " gene_pred_hmm_layer (GenePredH  (None, None, 15)    264         ['lstm_out[0][0]',               \n",
      " MMLayer)                                                         'tf.cast[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,048,303\n",
      "Trainable params: 8,048,039\n",
      "Non-trainable params: 264\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "pred_gtf.load_model()\n",
    "\n",
    "# load input data x_seq \n",
    "x_seq, y_seq, coords = pred_gtf.load_inp_data(    \n",
    "    strand=strand, \n",
    "    chunk_coords=True, softmask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f75c8a-b021-4b48-a30b-c65ddc58e524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LSTM prediction\n",
      "0 2 86 (500004, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 09:58:42.264144: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8800\n",
      "2024-11-02 09:58:42.505563: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-02 09:58:42.506771: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-02 09:58:42.506792: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-11-02 09:58:42.507574: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-02 09:58:42.507638: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-11-02 09:58:42.979819: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 86 (500004, 6)\n",
      "4 6 86 (500004, 6)\n",
      "6 8 86 (500004, 6)\n",
      "8 10 86 (500004, 6)\n",
      "10 12 86 (500004, 6)\n",
      "12 14 86 (500004, 6)\n",
      "14 16 86 (500004, 6)\n",
      "16 18 86 (500004, 6)\n",
      "18 20 86 (500004, 6)\n",
      "20 22 86 (500004, 6)\n",
      "22 24 86 (500004, 6)\n",
      "24 26 86 (500004, 6)\n",
      "26 28 86 (500004, 6)\n",
      "28 30 86 (500004, 6)\n",
      "30 32 86 (500004, 6)\n",
      "32 34 86 (500004, 6)\n",
      "34 36 86 (500004, 6)\n",
      "36 38 86 (500004, 6)\n",
      "38 40 86 (500004, 6)\n",
      "40 42 86 (500004, 6)\n",
      "42 44 86 (500004, 6)\n",
      "44 46 86 (500004, 6)\n",
      "46 48 86 (500004, 6)\n",
      "48 50 86 (500004, 6)\n",
      "50 52 86 (500004, 6)\n",
      "52 54 86 (500004, 6)\n",
      "54 56 86 (500004, 6)\n",
      "56 58 86 (500004, 6)\n",
      "58 60 86 (500004, 6)\n",
      "60 62 86 (500004, 6)\n",
      "62 64 86 (500004, 6)\n",
      "64 66 86 (500004, 6)\n",
      "66 68 86 (500004, 6)\n",
      "68 70 86 (500004, 6)\n",
      "70 72 86 (500004, 6)\n",
      "72 74 86 (500004, 6)\n",
      "74 76 86 (500004, 6)\n",
      "76 78 86 (500004, 6)\n",
      "78 80 86 (500004, 6)\n",
      "80 82 86 (500004, 6)\n",
      "82 84 86 (500004, 6)\n",
      "84 86 86 (500004, 6)\n",
      "LSTM took 2.0246428489685058 minutes to execute.\n",
      "### HMM Viterbi\n",
      "HMM took 0.45342657566070554 minutes to execute.\n",
      "### LSTM prediction\n",
      "0 1 3 (1000008, 6)\n",
      "1 2 3 (1000008, 6)\n",
      "2 3 3 (1000008, 6)\n",
      "LSTM took 0.23829193115234376 minutes to execute.\n",
      "### HMM Viterbi\n",
      "HMM took 0.03773817221323649 minutes to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<genome_anno.Anno at 0x7f6b706595d0>, 76)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate LSTM and HMM predictions\n",
    "hmm_pred = pred_gtf.get_predictions(x_seq, hmm_filter=True)\n",
    "\n",
    "# infer gene structures and write GTF file\n",
    "pred_gtf.create_gtf(y_label=hmm_pred, coords=coords,\n",
    "        out_file=gtf_out, f_chunks=x_seq, strand=strand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
