{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c85168-c1d0-4d13-ae5a-5131b50df3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 11:37:20.802210: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-24 11:37:20.905830: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-24 11:37:21.397977: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/beckerf/mambaforge/envs/tiberiusdev/lib/:/home/beckerf/mambaforge/envs/tiberiusdev/lib/python3.10/site-packages/nvidia/cudnn/lib:\n",
      "2024-10-24 11:37:21.398206: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/beckerf/mambaforge/envs/tiberiusdev/lib/:/home/beckerf/mambaforge/envs/tiberiusdev/lib/python3.10/site-packages/nvidia/cudnn/lib:\n",
      "2024-10-24 11:37:21.398217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-10-24 11:37:22.420807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.421050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.441625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.441873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.442071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.442257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.442724: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-24 11:37:22.552700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.552924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.553120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.553303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.553482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.553661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.559813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.560024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.560226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.560417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.560602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.560766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22463 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3e:00.0, compute capability: 8.6\n",
      "2024-10-24 11:37:22.561026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-24 11:37:22.561310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22463 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3f:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#temporary \n",
    "sys.path.insert(0, \"../../../learnMSA\")\n",
    "\n",
    "sys.path.append(\"../../bin\")\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as keras\n",
    "from eval_model_class import PredictionGTF\n",
    "from models import lstm_model, custom_cce_f1_loss, add_hmm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eab74a7-d772-4662-b9d1-ca904bdc726f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_len = 9999\n",
    "strand = '+'\n",
    "\n",
    "emb=False\n",
    "hmm_factor=1\n",
    "trans_lstm=False\n",
    "\n",
    "inp_data_dir = 'inp/'\n",
    "out_dir = 'test_train/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "genome_path = f'{inp_data_dir}/genome.fa'\n",
    "annot_path= f'{inp_data_dir}/annot.gtf'\n",
    "\n",
    "\n",
    "pred_gtf = PredictionGTF( \n",
    "     seq_len=seq_len, \n",
    "    batch_size=batch_size,\n",
    "    hmm=True, \n",
    "    emb=False, \n",
    "    num_hmm=1,\n",
    "    hmm_factor=1,\n",
    "    genome_path=genome_path,\n",
    "    annot_path=annot_path, \n",
    "    softmask=True, strand=strand,\n",
    ")\n",
    "\n",
    "# load input data x_seq \n",
    "x_seq, y_seq, _ = pred_gtf.load_inp_data(    \n",
    "    strand=strand, \n",
    "    chunk_coords=True, softmask=True, pad=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be965c83-fe3f-4a25-bbe5-317335e2618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " main_input (InputLayer)        [(None, None, 6)]    0           []                               \n",
      "                                                                                                  \n",
      " initial_conv (Conv1D)          (None, None, 32)     608         ['main_input[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization1 (LayerNor  (None, None, 32)    64          ['initial_conv[0][0]']           \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " conv_1 (Conv1D)                (None, None, 32)     9248        ['layer_normalization1[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization2 (LayerNor  (None, None, 32)    64          ['conv_1[0][0]']                 \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " conv_2 (Conv1D)                (None, None, 32)     9248        ['layer_normalization2[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 38)     0           ['main_input[0][0]',             \n",
      "                                                                  'conv_2[0][0]']                 \n",
      "                                                                                                  \n",
      " R1 (Reshape)                   (None, None, 342)    0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " pre_lstm_dense (Dense)         (None, None, 200)    68600       ['R1[0][0]']                     \n",
      "                                                                                                  \n",
      " biLSTM_1 (Bidirectional)       (None, None, 200)    240800      ['pre_lstm_dense[0][0]']         \n",
      "                                                                                                  \n",
      " biLSTM_2 (Bidirectional)       (None, None, 200)    240800      ['biLSTM_1[0][0]']               \n",
      "                                                                                                  \n",
      " out_dense (Dense)              (None, None, 135)    27135       ['biLSTM_2[0][0]']               \n",
      "                                                                                                  \n",
      " Reshape2 (Reshape)             (None, None, 15)     0           ['out_dense[0][0]']              \n",
      "                                                                                                  \n",
      " out (Activation)               (None, None, 15)     0           ['Reshape2[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_out (Identity)            (None, None, 15)     0           ['out[0][0]']                    \n",
      "                                                                                                  \n",
      " cast (Cast)                    (None, None, 5)      0           ['main_input[0][0]']             \n",
      "                                                                                                  \n",
      " gene_pred_hmm_layer (GenePredH  (None, None, 15)    264         ['lstm_out[0][0]',               \n",
      " MMLayer)                                                         'cast[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 596,831\n",
      "Trainable params: 596,567\n",
      "Non-trainable params: 264\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# see lstm_model documentation for more arguments \n",
    "config = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"stride\": 0,\n",
    "    \"units\": 100,\n",
    "    \"filter_size\": 32,\n",
    "    \"numb_lstm\": 2,\n",
    "    \"numb_conv\": 3,\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"hmm_dense\": 32,\n",
    "    \"pool_size\": 9,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"w_size\": seq_len,\n",
    "    'output_size': 15,\n",
    "    'hmm_loss_weight_mul': 0.1,\n",
    "    'hmm_emit_embeddings': False,\n",
    "    \"hmm_dense\": 32, # size of embedding for HMM input\n",
    "    'hmm_share_intron_parameters': False,\n",
    "    'hmm_nucleotides_at_exons': False,\n",
    "    'hmm_trainable_transitions': False,\n",
    "    'hmm_trainable_starting_distribution': False,\n",
    "    'hmm_trainable_emissions': False, #does not affect embedding emissions, use hmm_emit_embeddings for that\n",
    "    \"neutral_hmm\": False, # initializes an HMM without human expert bias, currently not implemented\n",
    "    'hmm_factor': 99,    \n",
    "    'initial_variance': 0.1,\n",
    "    'temperature': 32*3,\n",
    "    \"loss_f1_factor\": 2.0,\n",
    "}\n",
    "\n",
    "relevant_keys = ['units', 'filter_size', 'kernel_size', \n",
    "                     'numb_conv', 'numb_lstm', 'dropout_rate', \n",
    "                     'pool_size', 'stride',  \n",
    "                     'output_size', 'multi_loss']\n",
    "\n",
    "relevant_args = {key: config[key] for key in relevant_keys if key in config}\n",
    "model = lstm_model(**relevant_args)\n",
    "model = add_hmm_layer(model, None,\n",
    "                dense_size=config['hmm_dense'], \n",
    "                pool_size=config['pool_size'],\n",
    "                output_size=config['output_size'], \n",
    "                num_hmm=1,\n",
    "                l2_lambda=0.,\n",
    "                hmm_factor=config['hmm_factor'], \n",
    "                batch_size=config['batch_size'],\n",
    "                seq_len=config['w_size'],\n",
    "                initial_variance=config['initial_variance'],\n",
    "                temperature=config['temperature'],\n",
    "                emit_embeddings=False, \n",
    "              share_intron_parameters=config['hmm_share_intron_parameters'],\n",
    "                trainable_nucleotides_at_exons=config['hmm_nucleotides_at_exons'],\n",
    "                trainable_emissions=config['hmm_trainable_emissions'],\n",
    "                trainable_transitions=config['hmm_trainable_transitions'],\n",
    "                trainable_starting_distribution=config['hmm_trainable_starting_distribution'],\n",
    "                use_border_hints=False,\n",
    "                include_lstm_in_output=False,\n",
    "                neutral_hmm=config['neutral_hmm'])\n",
    "              \n",
    "adam = Adam(learning_rate=config[\"lr\"])\n",
    "f1loss = custom_cce_f1_loss(config[\"loss_f1_factor\"], batch_size=config[\"batch_size\"])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=f1loss, optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e744f76-5810-48f1-993e-5f7cbe026db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 11:37:35.702880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8800\n",
      "2024-10-24 11:37:35.791551: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-24 11:37:35.792645: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-24 11:37:35.792662: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-10-24 11:37:35.793923: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-24 11:37:35.793984: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-10-24 11:37:35.817216: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 154/1057 [===>..........................] - ETA: 5:54 - loss: -1193.0193 - accuracy: 0.0381 - prior: -0.1017"
     ]
    }
   ],
   "source": [
    "model.fit(x=x_seq, y=y_seq, \n",
    "          epochs=config[\"num_epochs\"], \n",
    "          batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ace38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
