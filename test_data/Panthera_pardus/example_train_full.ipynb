{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c85168-c1d0-4d13-ae5a-5131b50df3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, tarfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as keras\n",
    "from tiberius.write_tfrecord_species import get_species_data_hmm\n",
    "from tiberius.models import lstm_model, custom_cce_f1_loss, add_hmm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab74a7-d772-4662-b9d1-ca904bdc726f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "seq_len = 9999\n",
    "strand = '+'\n",
    "\n",
    "hmm_factor=1\n",
    "\n",
    "# extract test_data if necassary\n",
    "inp_data_dir = 'inp/'\n",
    "if not os.path.exists(inp_data_dir):\n",
    "    os.mkdir(inp_data_dir)  \n",
    "    with tarfile.open(\"inp.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall(path=inp_data_dir)\n",
    "\n",
    "out_dir = 'test_train/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "genome_path = f'{inp_data_dir}/genome.fa'\n",
    "annot_path= f'{inp_data_dir}/annot.gtf'\n",
    "\n",
    "# load input data x_seq \n",
    "x_seq, y_seq = get_species_data_hmm(annot_path=annot_path, genome_path=genome_path,\n",
    "    seq_len=seq_len, transition=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be965c83-fe3f-4a25-bbe5-317335e2618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see lstm_model documentation for more arguments \n",
    "config = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"stride\": 0,\n",
    "    \"units\": 100,\n",
    "    \"filter_size\": 32,\n",
    "    \"numb_lstm\": 2,\n",
    "    \"numb_conv\": 3,\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"pool_size\": 9,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"w_size\": seq_len,\n",
    "    'output_size': 15,\n",
    "    'hmm_share_intron_parameters': False,\n",
    "    'hmm_nucleotides_at_exons': False,\n",
    "    'hmm_trainable_transitions': False,\n",
    "    'hmm_trainable_starting_distribution': False,\n",
    "    'hmm_trainable_emissions': False, \n",
    "    'hmm_factor': 99,    \n",
    "    \"loss_f1_factor\": 2.0,\n",
    "}\n",
    "\n",
    "relevant_keys = ['units', 'filter_size', 'kernel_size', \n",
    "                     'numb_conv', 'numb_lstm', 'dropout_rate', \n",
    "                     'pool_size', 'stride',  \n",
    "                     'output_size', 'multi_loss']\n",
    "\n",
    "relevant_args = {key: config[key] for key in relevant_keys if key in config}\n",
    "model = lstm_model(**relevant_args)\n",
    "model = add_hmm_layer(model,  \n",
    "                output_size=config['output_size'], \n",
    "                num_hmm=1,\n",
    "                hmm_factor=config['hmm_factor'], \n",
    "                share_intron_parameters=config['hmm_share_intron_parameters'],\n",
    "                trainable_nucleotides_at_exons=config['hmm_nucleotides_at_exons'],\n",
    "                trainable_emissions=config['hmm_trainable_emissions'],\n",
    "                trainable_transitions=config['hmm_trainable_transitions'],\n",
    "                trainable_starting_distribution=config['hmm_trainable_starting_distribution'],\n",
    "                include_lstm_in_output=False,)\n",
    "              \n",
    "adam = Adam(learning_rate=config[\"lr\"])\n",
    "f1loss = custom_cce_f1_loss(config[\"loss_f1_factor\"], batch_size=config[\"batch_size\"], from_logits=True)\n",
    "\n",
    "model.compile(loss=f1loss, optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e744f76-5810-48f1-993e-5f7cbe026db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=x_seq, y=y_seq, \n",
    "          epochs=config[\"num_epochs\"], \n",
    "          batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ace38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf_version > '2.12':\n",
    "    model.save(f\"{out_dir}/test_train_hmm.keras\")\n",
    "else:\n",
    "    model.save(f\"{out_dir}/test_train_hmm\", save_traces=False)\n",
    "\n",
    "# the trained model can be used with tiberius using the --model_old option\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
